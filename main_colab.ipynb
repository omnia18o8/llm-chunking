{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NnamdiOdozi/Intern_GenAI_Examples/blob/main/main_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e57dea9c",
      "metadata": {
        "id": "e57dea9c"
      },
      "outputs": [],
      "source": [
        "# Force IPython to auto-flush outputs\n",
        "%config Application.log_level = 'INFO'\n",
        "%config InteractiveShell.ast_node_interactivity = \"all\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "192a88d3",
        "outputId": "1e311909-d023-4fd5-e924-282b516efbc6"
      },
      "source": [
        "%pip install --upgrade --quiet langchain langchain-core langchain-openai openai tiktoken\n"
      ],
      "id": "192a88d3",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/107.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "20df530d",
      "metadata": {
        "id": "20df530d"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import io\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea354f2e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea354f2e",
        "outputId": "fb0df31f-688a-4f04-ad5f-5fa8b0e8faea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[ColabKernelApp] Aborting 3c590369-6dab-4ebb-cd89-5c0524f73c49: execute_request\n"
          ]
        }
      ],
      "source": [
        "# Load environment variables\n",
        "# from dotenv import load_dotenv\n",
        "# load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "\n",
        "# Access the API key from Colab secrets\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# You can now use the OPENAI_API_KEY variable to configure your OpenAI client.\n",
        "# For example:\n",
        "# client = OpenAI(api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "XvNsg5-i6NtB"
      },
      "id": "XvNsg5-i6NtB",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "53088d0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53088d0b",
        "outputId": "7718791f-58c2-40b2-e50e-a5f5921b39a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As of June 2024, the President of the United States is Joe Biden.\n",
            "content='As of June 2024, the President of the United States is Joe Biden.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 25, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'id': 'chatcmpl-CTXLFXYBtGBRBu5EGRUgzzlZBeSI6', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--465fb032-f89f-4a15-915f-adac5676b1f7-0' usage_metadata={'input_tokens': 25, 'output_tokens': 17, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "# Single LLM Call\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4.1-mini\",\n",
        "    temperature=0.1,\n",
        "    api_key= OPENAI_API_KEY,\n",
        "    request_timeout=120,\n",
        "    max_retries=5,\n",
        "    verbose=False,\n",
        "\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    (\"assistant\", \"You are a helpful assistant.\"),\n",
        "    (\"human\", \"Who is the president of the USA.\")\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "res = llm.invoke(messages)   # list-of-conversations -> ChatResult\n",
        "\n",
        "print(res.content)\n",
        "\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bcfadaa0",
      "metadata": {
        "id": "bcfadaa0",
        "outputId": "a57957c0-90d8-419d-e67e-26d55aebb3fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"J'aime la programmation.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
        "    ),\n",
        "    (\"human\", \"I love programming.\"),\n",
        "]\n",
        "ai_msg = llm.invoke(messages)\n",
        "ai_msg.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "844b5ae3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "844b5ae3",
        "outputId": "9fb76102-5f9a-44ea-aa97-b9496492967e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 18, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'id': 'chatcmpl-CTXGgNUh52iejAV5i18aNLu3SPEe2', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--440e7356-1dde-4a69-b508-21ca3920cdaf-0', usage_metadata={'input_tokens': 18, 'output_tokens': 9, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), AIMessage(content='Crimson leaves descend,  \\nWhispers of the cooling breeze,  \\nAutumn’s soft farewell.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 16, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'id': 'chatcmpl-CTXGffc4ZptZSNEWOp3lp9tmuwCSD', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--e9164cac-062a-4a3f-98fc-2b306dd6c435-0', usage_metadata={'input_tokens': 16, 'output_tokens': 20, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "CHOICE 1\n",
            "\n",
            "Hello! How can I assist you today?\n",
            "\n",
            "\n",
            "CHOICE 2\n",
            "\n",
            "Crimson leaves descend,  \n",
            "Whispers of the cooling breeze,  \n",
            "Autumn’s soft farewell.\n"
          ]
        }
      ],
      "source": [
        "if True:\n",
        "    # Batched LLM Calls\n",
        "    from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "    messages = [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        ([HumanMessage(\"Write me a haiku about autumn leaves.\")])\n",
        "    ] * 1\n",
        "\n",
        "    completion = llm.batch(messages)  # list-of-list-of-conversations -> ChatResult\n",
        "\n",
        "    print(completion)\n",
        "\n",
        "    for i, choice in enumerate(completion):\n",
        "        print(f\"\\n\\nCHOICE {i+1}\\n\")\n",
        "        print(choice.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "5cbf2e54",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cbf2e54",
        "outputId": "a044341d-9cb0-4277-9929-38173d3a637b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "J'aime la programmation.\n"
          ]
        }
      ],
      "source": [
        "# Using template allows use insert parameters into the prompt - you only need to provide parameter values at query time\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
        "        ),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | llm\n",
        "res = chain.invoke(\n",
        "    {\n",
        "        \"input_language\": \"English\",\n",
        "        \"output_language\": \"French\",\n",
        "        \"input\": \"I love programming.\",\n",
        "    }\n",
        ")\n",
        "\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e572ccc8",
      "metadata": {
        "id": "e572ccc8",
        "outputId": "9cd67fa7-7341-4b82-ff04-8b51a294d5ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Input prompt: hey what is chatprompttemplate used for???\n",
            "Sure! ChatPromptTemplate is a tool used to create structured prompts for chat-based AI models. It helps you organize and format the input messages clearly, making it easier to guide the AI’s responses. By using it, you can define roles (like user or assistant), insert variables, and build dynamic conversations efficiently. It’s great for crafting consistent and effective prompts in chat applications!\n",
            "\n",
            " Before reformulation: hey what is chatprompttemplate used for???\n",
            "\n",
            " After reformulation: Could you please explain the purpose and use of ChatPromptTemplate?\n",
            "\n",
            " After answering: Sure! ChatPromptTemplate is a tool used to create structured prompts for chat-based AI models. It helps you organize and format the input messages clearly, making it easier to guide the AI’s responses. By using it, you can define roles (like user or assistant), insert variables, and build dynamic conversations efficiently. It’s great for crafting consistent and effective prompts in chat applications!\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.0, api_key=OPENAI_API_KEY)\n",
        "\n",
        "# --- First prompt: reformulate ---\n",
        "reformulate_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant that rewrites questions in a clearer, more formal way.\"),\n",
        "    (\"user\", \"Rewrite the following question to be clear and formal: {raw_question}\")\n",
        "])\n",
        "\n",
        "# --- Second prompt: answer the reformulated question ---\n",
        "answer_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a knowledgeable tutor.\"),\n",
        "    (\"user\", \"Answer this question in a concise and friendly way:\\n\\n{clean_question}\")\n",
        "])\n",
        "\n",
        "# --- Chain them together ---\n",
        "# reformulate → llm → inject into second prompt → llm\n",
        "chain = (\n",
        "    reformulate_prompt\n",
        "    | llm\n",
        "    | (lambda msg: {\"clean_question\": msg.content})  # extract content\n",
        "    | answer_prompt\n",
        "    | llm\n",
        ")\n",
        "\n",
        "# --- Run the chain ---\n",
        "query = \"hey what is chatprompttemplate used for???\"\n",
        "response = chain.invoke({\"raw_question\": query})\n",
        "\n",
        "print(\"\\n Input prompt:\", query)\n",
        "print(response.content)\n",
        "\n",
        "# Run only the first prompt + LLM\n",
        "first_result = (reformulate_prompt | llm).invoke({\"raw_question\": query})\n",
        "print(\"\\n Before reformulation:\", query)\n",
        "print(\"\\n After reformulation:\", first_result.content)\n",
        "\n",
        "second_result = (answer_prompt | llm).invoke({\"clean_question\": first_result.content})\n",
        "print(\"\\n After answering:\", second_result.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51169d16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51169d16",
        "outputId": "57cd7142-af33-43ad-b5ed-1ee9e51ed307"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q1: who won the world cup in 2022? \n",
            "Assistant: Argentina won the 2022 FIFA World Cup. They defeated France in the final, which took place on December 18, 2022, in Qatar. The match ended in a dramatic penalty shootout after a 3-3 draw in regulation and extra time.\n",
            "\n",
            "Q2: who was the top scorer in the competition?\n",
            "Assistant: Could you please specify which competition you are referring to? There are many sports and events with top scorers, so I need more details to provide an accurate answer.\n",
            "\n",
            "Q3: what is the capital of France?\n",
            "Assistant: The capital of France is Paris.\n",
            "\n",
            "Q4: \n"
          ]
        }
      ],
      "source": [
        "if True:\n",
        "\n",
        "    # how to have an interactive real time conversation with the assistant?\n",
        "\n",
        "    # Initialise model\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0, api_key=OPENAI_API_KEY)\n",
        "\n",
        "    def ask_once(user_input: str):\n",
        "        # Only system + user, no history\n",
        "        messages = [\n",
        "            SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "            HumanMessage(content=user_input)\n",
        "        ]\n",
        "        response = llm.invoke(messages)\n",
        "        return response.content\n",
        "\n",
        "    # Run a simple q&a session\n",
        "    i=0\n",
        "    while True:\n",
        "        user_input = input(f\"Q{i+1}: \")   # prompt user in Jupyter cell\n",
        "        if user_input == \"quit\" or user_input == \"exit\" or user_input == \"break\" or not user_input.strip():        # allow empty string or quit or exit or break to break early\n",
        "            break\n",
        "        answer = ask_once(user_input)\n",
        "        i += 1\n",
        "        print(f\"Assistant: {answer}\\n\")\n",
        "\n",
        "        # Example questions are: where were the last olympics held?  who won the world cup in 2022?  Who wa the team captain? what is the capital of france? who is the president of the united states? what is the tallest mountain in the world?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c0ecfb1",
      "metadata": {
        "id": "8c0ecfb1"
      },
      "source": [
        "What did you notice about this Assistant?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b1ae6c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b1ae6c6",
        "outputId": "48e1df6d-293d-4be8-8971-3754e0b52996"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q1: who won the world cup in 2022? \n",
            "Assistant: Argentina won the 2022 FIFA World Cup. They defeated France in the final, which took place on December 18, 2022, in Qatar. The match ended in a 3-3 draw after extra time, and Argentina won 4-2 in the penalty shootout.\n",
            "\n",
            "Q2: who was the top scorer in the competition?\n",
            "Assistant: The top scorer of the 2022 FIFA World Cup was Kylian Mbappé of France. He scored a total of 8 goals during the tournament, including a hat-trick in the final against Argentina.\n",
            "\n",
            "Q3: who was best player of the tournament?\n",
            "Assistant: The best player of the 2022 FIFA World Cup, awarded the Golden Ball, was Lionel Messi of Argentina. He played a crucial role in leading his team to victory and had an outstanding tournament, scoring 7 goals and providing key assists.\n",
            "\n",
            "Q4: who was the 2nd highest goal scorer?\n",
            "Assistant: The second highest goal scorer of the 2022 FIFA World Cup was Lionel Messi, who scored 7 goals during the tournament. Kylian Mbappé was the top scorer with 8 goals.\n",
            "\n",
            "Q5: \n"
          ]
        }
      ],
      "source": [
        "if True:\n",
        "\n",
        "    # What is different about this version?\n",
        "\n",
        "    # initialise model\n",
        "    #llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
        "\n",
        "    # start with a system prompt\n",
        "    history = [SystemMessage(content=\"You are a helpful assistant.\")]\n",
        "\n",
        "    # function to ask user input and continue the conversation\n",
        "    def chat_turn(user_input: str):\n",
        "        # append user message\n",
        "        history.append(HumanMessage(content=user_input))\n",
        "\n",
        "        # call model with full history\n",
        "        ai_msg = llm.invoke(history)  # returns an AIMessage\n",
        "        #print(f\"Assistant: {ai_msg.content}\")\n",
        "\n",
        "        # append assistant reply manually (so it's fed into the next turn)\n",
        "        history.append(ai_msg)\n",
        "        return ai_msg.content\n",
        "\n",
        "    # Run a simple q&a session\n",
        "    i=0\n",
        "    while True:\n",
        "        user_input = input(f\"Q{i+1}: \")   # prompt user in Jupyter cell\n",
        "        if user_input == \"quit\" or user_input == \"exit\" or user_input == \"break\" or not user_input.strip():        # allow empty string or quit or exit or break to break early\n",
        "            break\n",
        "        answer = chat_turn(user_input)\n",
        "        i += 1\n",
        "        print(f\"Assistant: {answer}\\n\")\n",
        "\n",
        "        # Example questions are: where were the last olympics held?  who won the world cup in 2022?  Who wa the team captain? what is the capital of france? who is the president of the united states? what is the tallest mountain in the world?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e5f4f87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e5f4f87",
        "outputId": "d01cb86b-fe7f-4c33-af6e-be3edb66601d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q1: who is the president of the USA?\n",
            "Assistant: As of September 22, 2025, the President of the United States is Donald Trump. He was inaugurated as the 47th president on January 20, 2025, after defeating Kamala Harris in the 2024 presidential election. This marks his second term in office, having previously served as the 45th president from 2017 to 2021. ([en.wikipedia.org](https://en.wikipedia.org/wiki/Second_presidency_of_Donald_Trump?utm_source=openai))\n",
            "\n",
            "In the 2024 election, Trump, a Republican, faced Harris, the Democratic nominee and then-Vice President under President Joe Biden. Trump's victory in the 2024 election led to his re-inauguration in January 2025. ([apnews.com](https://apnews.com/article/d69ed8609cbb7c38c1f0fe1e074027f2?utm_source=openai))\n",
            "\n",
            "During his first term, Trump faced significant challenges, including impeachments, criminal indictments, and assassination attempts. Despite these obstacles, he secured re-election in 2024, overcoming a competitive race against Harris. ([apnews.com](https://apnews.com/article/d69ed8609cbb7c38c1f0fe1e074027f2?utm_source=openai))\n",
            "\n",
            "\n",
            "## Donald Trump Inaugurated as 47th President of the United States:\n",
            "- [Inauguration Day Latest: Trump becomes the 47th president of the United States](https://apnews.com/article/d69ed8609cbb7c38c1f0fe1e074027f2?utm_source=openai) \n",
            "\n",
            "{\"As of September 22, 2025, the President of the United States is Donald Trump. He was inaugurated as the 47th president on January 20, 2025, after defeating Kamala Harris in the 2024 presidential election. This marks his second term in office, having previously served as the 45th president from 2017 to 2021. ([en.wikipedia.org](https://en.wikipedia.org/wiki/Second_presidency_of_Donald_Trump?utm_source=openai))\\n\\nIn the 2024 election, Trump, a Republican, faced Harris, the Democratic nominee and then-Vice President under President Joe Biden. Trump's victory in the 2024 election led to his re-inauguration in January 2025. ([apnews.com](https://apnews.com/article/d69ed8609cbb7c38c1f0fe1e074027f2?utm_source=openai))\\n\\nDuring his first term, Trump faced significant challenges, including impeachments, criminal indictments, and assassination attempts. Despite these obstacles, he secured re-election in 2024, overcoming a competitive race against Harris. ([apnews.com](https://apnews.com/article/d69ed8609cbb7c38c1f0fe1e074027f2?utm_source=openai))\\n\\n\\n## Donald Trump Inaugurated as 47th President of the United States:\\n- [Inauguration Day Latest: Trump becomes the 47th president of the United States](https://apnews.com/article/d69ed8609cbb7c38c1f0fe1e074027f2?utm_source=openai) \"}\n",
            "Q2: \n"
          ]
        }
      ],
      "source": [
        "if True:\n",
        "    # And about this version?\n",
        "\n",
        "    # initialise model\n",
        "    #llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0, output_version=\"responses/v1\")\n",
        "\n",
        "    tool = {\"type\": \"web_search_preview\"}\n",
        "    llm_with_tools = llm.bind_tools([tool])\n",
        "\n",
        "    # start with a system prompt\n",
        "    history = [SystemMessage(content=\"You are a helpful assistant.\")]\n",
        "\n",
        "    # function to ask user input and continue the conversation\n",
        "    def chat_turn(user_input: str):\n",
        "        # append user message\n",
        "        history.append(HumanMessage(content=user_input))\n",
        "\n",
        "        # call model with full history\n",
        "        ai_msg = llm_with_tools.invoke(history)  # returns an AIMessage\n",
        "        #print(f\"Assistant: {ai_msg.content}\")\n",
        "\n",
        "        # append assistant reply manually (so it's fed into the next turn)\n",
        "        history.append(ai_msg)\n",
        "        return ai_msg.text()\n",
        "\n",
        "    # Run a simple q&a session\n",
        "    i=0\n",
        "    while True:\n",
        "        user_input = input(f\"Q{i+1}: \")   # prompt user in Jupyter cell\n",
        "        if user_input == \"quit\" or user_input == \"exit\" or user_input == \"break\" or not user_input.strip():        # allow empty string or quit or exit or break to break early\n",
        "            break\n",
        "        answer = chat_turn(user_input)\n",
        "        i += 1\n",
        "        print(f\"Assistant: {answer}\\n\")\n",
        "        print({answer})\n",
        "\n",
        "        # Example questions are: where were the last olympics held?  who won the world cup in 2022?  Who wa the team captain? what is the capital of france? who is the president of the united states? what is the tallest mountain in the world?\n",
        "        # who won the women's 100m  hurldes at the 2025 tokyo world athletics championship"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edbb4f2c",
      "metadata": {
        "id": "edbb4f2c",
        "outputId": "728880e8-637f-485f-9c05-fa7b9573d23b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "print(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae42a4e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae42a4e2",
        "outputId": "5de484bf-7b89-4715-da78-3ef0fbaeb23b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"answer\":\"A pound of feathers and a pound of gold weigh the same, as both are one pound.\",\"justification\":\"Weight is a measure of mass, and one pound is equal to one pound regardless of the material being weighed. Therefore, a pound of feathers and a pound of gold both weigh exactly one pound.\"}\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "def get_weather(location: str) -> None:\n",
        "    \"\"\"Get weather at a location.\"\"\"\n",
        "    return \"It's sunny.\"\n",
        "\n",
        "\n",
        "class OutputSchema(BaseModel):\n",
        "    \"\"\"Schema for response.\"\"\"\n",
        "\n",
        "    answer: str\n",
        "    justification: str\n",
        "\n",
        "\n",
        "#llm = ChatOpenAI(model=\"gpt-4.1\")\n",
        "\n",
        "structured_llm = llm.bind_tools(\n",
        "    [get_weather],\n",
        "    response_format=OutputSchema,\n",
        "    strict=True,\n",
        ")\n",
        "\n",
        "# Response contains tool calls:\n",
        "tool_call_response = structured_llm.invoke(\"What is the weather in SF?\")\n",
        "\n",
        "# structured_response.additional_kwargs[\"parsed\"] contains parsed output\n",
        "structured_response = structured_llm.invoke(\n",
        "    \"What weighs more, a pound of feathers or a pound of gold?\"\n",
        ")\n",
        "print(structured_response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22fd42a9",
      "metadata": {
        "id": "22fd42a9"
      },
      "outputs": [],
      "source": [
        "# Example of a tool call\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_weather\",\n",
        "            \"description\": \"Get the weather for a city\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\"city\": {\"type\": \"string\"}},\n",
        "                \"required\": [\"city\"],\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "    model=\"gpt-4.1-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"What's the weather in London?\"}],\n",
        "    tools=tools,\n",
        "    tool_choice=\"auto\",\n",
        ")\n",
        "\n",
        "print(resp.choices[0].message)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "888010fc",
      "metadata": {
        "id": "888010fc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Intern_GenAI_Examples",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}