{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec96d834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import voyageai as vo  \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "client_vo = vo.Client(api_key=os.getenv(\"VOYAGE_API_KEY\"))  \n",
    "chat_evaluator_model = \"gpt-5-nano\"\n",
    "chat_judge_model = \"gpt-4.1-mini\"\n",
    "embedding_model = \"voyage-context-3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d1cacd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "a = 0.6 #alpha for semantic (1.0) and keyword search (0.0)\n",
    "lambda_parameter = 0.7 #lambda for diversity (1.0 is all diverse) and 0.0 is chunks are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef4a0cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28 documents.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ingestion import load_documents\n",
    "\n",
    "base_path = os.path.abspath(\"..\")\n",
    "md_texts = load_documents(base_path)\n",
    "\n",
    "print(f\"Loaded {len(md_texts)} documents.\") #check to see documents have been loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cb5b67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 total chunks created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" \n",
    "First we have to split the documents into chunks to be able to group them contextually together later.\n",
    "The first decision here is to choose what type of chunking to use for now. \n",
    "I tried CTS but the issue with that is that it would split mid praragraph/sentence, \n",
    "which meant we would loose coherence sometimes. \n",
    "Using a recursive CTS meant we have more chunks as well, as it makes sure to perserve context. \n",
    "Chunk size here is really small because we later on attach 4 chunks to it when cintextualising,\n",
    "this means that at the end of chunking, a chunk will have around 640 tokens\"\"\"\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "all_chunks = []\n",
    "\n",
    "for doc in md_texts:\n",
    "    chunks = splitter.create_documents([doc[\"text\"]])\n",
    "    for c in chunks:\n",
    "        c.metadata = {\"source\": doc[\"file\"]}\n",
    "    all_chunks.extend(chunks)\n",
    "    \n",
    "print(len(all_chunks), \"total chunks created\") \n",
    "#print(all_chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5838a952",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "Model voyage-3-large is not supported. Supported models are ['voyage-context-3'].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidRequestError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m chunk_groups = \u001b[38;5;28mlist\u001b[39m(batch_chunks(all_chunks, GROUP_SIZE))\n\u001b[32m     34\u001b[39m inputs = [[c.page_content \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m group] \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m chunk_groups]\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m response = \u001b[43mclient_vo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontextualized_embed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocument\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     40\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group, doc_result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(chunk_groups, response.results):\n\u001b[32m     44\u001b[39m     embeddings = [np.array(e, dtype=np.float32) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m doc_result.embeddings]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Intern_GenAI_Examples/.venv/lib/python3.13/site-packages/voyageai/client.py:98\u001b[39m, in \u001b[36mClient.contextualized_embed\u001b[39m\u001b[34m(self, inputs, model, input_type, output_dtype, output_dimension, chunk_fn)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcontextualized_embed\u001b[39m(\n\u001b[32m     88\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     89\u001b[39m     inputs: List[List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m     chunk_fn: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], List[\u001b[38;5;28mstr\u001b[39m]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     95\u001b[39m ) -> ContextualizedEmbeddingsObject:\n\u001b[32m     97\u001b[39m     response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattempt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_controller\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattempt\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk_fn\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Intern_GenAI_Examples/.venv/lib/python3.13/site-packages/tenacity/__init__.py:443\u001b[39m, in \u001b[36mBaseRetrying.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    441\u001b[39m retry_state = RetryCallState(\u001b[38;5;28mself\u001b[39m, fn=\u001b[38;5;28;01mNone\u001b[39;00m, args=(), kwargs={})\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    445\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m AttemptManager(retry_state=retry_state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Intern_GenAI_Examples/.venv/lib/python3.13/site-packages/tenacity/__init__.py:376\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    374\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Intern_GenAI_Examples/.venv/lib/python3.13/site-packages/tenacity/__init__.py:398\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    399\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Intern_GenAI_Examples/.venv/lib/python3.13/site-packages/voyageai/client.py:102\u001b[39m, in \u001b[36mClient.contextualized_embed\u001b[39m\u001b[34m(self, inputs, model, input_type, output_dtype, output_dimension, chunk_fn)\u001b[39m\n\u001b[32m    100\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m chunk_fn:\n\u001b[32m    101\u001b[39m             inputs = apply_chunking(inputs, chunk_fn)\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m         response = \u001b[43mvoyageai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mContextualizedEmbedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_dimension\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dimension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error.APIConnectionError(\u001b[33m\"\u001b[39m\u001b[33mFailed to get response after all retry attempts\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Intern_GenAI_Examples/.venv/lib/python3.13/site-packages/voyageai/api_resources/contextualized_embedding.py:19\u001b[39m, in \u001b[36mContextualizedEmbedding.create\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m user_provided_encoding_format:\n\u001b[32m     17\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoding_format\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mbase64\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# If a user specifies base64, we'll just return the encoded string.\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# This is only for the default case.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m user_provided_encoding_format:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Intern_GenAI_Examples/.venv/lib/python3.13/site-packages/voyageai/api_resources/api_resource.py:47\u001b[39m, in \u001b[36mAPIResource.create\u001b[39m\u001b[34m(cls, api_key, api_base, request_id, request_timeout, **params)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m     **params,\n\u001b[32m     42\u001b[39m ):\n\u001b[32m     43\u001b[39m     requestor, url, params, headers = \u001b[38;5;28mcls\u001b[39m.__prepare_create_request(\n\u001b[32m     44\u001b[39m         api_key, api_base, **params\n\u001b[32m     45\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     response = \u001b[43mrequestor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m     obj = convert_to_voyage_response(response)\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Intern_GenAI_Examples/.venv/lib/python3.13/site-packages/voyageai/api_resources/api_requestor.py:147\u001b[39m, in \u001b[36mAPIRequestor.request\u001b[39m\u001b[34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    128\u001b[39m     method,\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    136\u001b[39m ) -> VoyageHttpResponse:\n\u001b[32m    137\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.request_raw(\n\u001b[32m    138\u001b[39m         method.lower(),\n\u001b[32m    139\u001b[39m         url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    145\u001b[39m         request_timeout=request_timeout,\n\u001b[32m    146\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Intern_GenAI_Examples/.venv/lib/python3.13/site-packages/voyageai/api_resources/api_requestor.py:408\u001b[39m, in \u001b[36mAPIRequestor._interpret_response\u001b[39m\u001b[34m(self, result)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_interpret_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, result: requests.Response) -> VoyageHttpResponse:\n\u001b[32m    406\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns the response(s) and a bool indicating whether it is a stream.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Intern_GenAI_Examples/.venv/lib/python3.13/site-packages/voyageai/api_resources/api_requestor.py:463\u001b[39m, in \u001b[36mAPIRequestor._interpret_response_line\u001b[39m\u001b[34m(self, rbody, rcode, rheaders)\u001b[39m\n\u001b[32m    461\u001b[39m resp = VoyageHttpResponse(data, rheaders)\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= rcode < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handle_error_response(rbody, rcode, resp.data, rheaders)\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[31mInvalidRequestError\u001b[39m: Model voyage-3-large is not supported. Supported models are ['voyage-context-3']."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script performs contextualized semantic embedding using Voyage's `voyage-context-3` model.\n",
    "\n",
    "What it does:\n",
    "- Splits all chunks into smaller pseudo-documents (default: 15 chunks per group)\n",
    "- Sends each group to the `contextualized_embed` API to get document-aware embeddings\n",
    "- For each chunk, finds its top-k most semantically similar chunks (within the same group)\n",
    "- Builds an expanded version of the chunk by appending its similar neighbors\n",
    "- Stores both the original chunk, expanded context, document ID, and normalized embedding\n",
    "\n",
    "Optimizations & decisions:\n",
    "- We avoid grouping by actual document to allow flexibility and even similarity across unrelated chunks\n",
    "- Group size (`GROUP_SIZE`) is tuned to stay below the 32k token limit of `voyage-context-3`\n",
    "- Embeddings are normalized to enable cosine similarity and MMR-style ranking\n",
    "- Top-k (`TOP_K`) controls how much semantic context is appended â€” small enough to keep expansion relevant\n",
    "- Processing is batched efficiently to reduce API calls while respecting model constraints\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "TOP_K = 2\n",
    "GROUP_SIZE = 15\n",
    "\n",
    "def batch_chunks(chunks, size):\n",
    "    for i in range(0, len(chunks), size):\n",
    "        yield chunks[i:i + size]\n",
    "\n",
    "contextual_all_chunks = []\n",
    "\n",
    "# Prepare all chunk groups for embedding\n",
    "chunk_groups = list(batch_chunks(all_chunks, GROUP_SIZE))\n",
    "inputs = [[c.page_content for c in group] for group in chunk_groups]\n",
    "\n",
    "response = client_vo.contextualized_embed(\n",
    "    inputs=inputs,\n",
    "    model=embedding_model,\n",
    "    input_type=\"document\"\n",
    ")\n",
    "\n",
    "\n",
    "for group, doc_result in zip(chunk_groups, response.results):\n",
    "    embeddings = [np.array(e, dtype=np.float32) for e in doc_result.embeddings]\n",
    "    normed_embeddings = [v / np.linalg.norm(v) for v in embeddings]\n",
    "    texts = [c.page_content for c in group]\n",
    "\n",
    "    for i, (chunk_text, vec) in enumerate(zip(texts, normed_embeddings)):\n",
    "        sims = cosine_similarity([vec], normed_embeddings)[0]\n",
    "        sims[i] = -1\n",
    "        top_k_idx = sims.argsort()[-TOP_K:]\n",
    "        similar_texts = [texts[j] for j in top_k_idx]\n",
    "        expanded_text = chunk_text + \" \" + \" \".join(similar_texts)\n",
    "\n",
    "        contextual_all_chunks.append({\n",
    "            \"chunk\": chunk_text,\n",
    "            \"expanded\": expanded_text,\n",
    "            \"doc_id\": group[i].metadata[\"source\"],\n",
    "            \"embedding\": vec\n",
    "        })\n",
    "\n",
    "chunk_texts = [c[\"chunk\"] for c in contextual_all_chunks]\n",
    "expanded_texts = [c[\"expanded\"] for c in contextual_all_chunks]\n",
    "chunk_embeddings = np.array([c[\"embedding\"] for c in contextual_all_chunks], dtype=np.float32)\n",
    "\n",
    "print(f\"\\n Total contextualized chunks created: {len(contextual_all_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag import response_llm, embed_query\n",
    "import json\n",
    "\n",
    "with open(\"qa_long_dataset.json\", \"r\") as f:\n",
    "    qa_data = json.load(f)\n",
    "questions = [q[\"question\"] for q in qa_data]\n",
    "\n",
    "# Run\n",
    "llm_results = response_llm(\n",
    "    questions=questions,\n",
    "    client=client,\n",
    "    chat_model=chat_evaluator_model,\n",
    "    chunk_embeddings=chunk_embeddings,\n",
    "    chunk_texts=chunk_texts,\n",
    "    embed_query=embed_query,\n",
    "    k=k,\n",
    "    alpha=a,\n",
    "    lambda_param=lambda_parameter\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b113e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"qa_long_dataset.json\", \"r\") as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "ragas_data = []\n",
    "for ref, pred in zip(qa_data, llm_results):\n",
    "    ragas_data.append({\n",
    "        \"question\": ref[\"question\"],\n",
    "        \"answer\": pred[\"answer\"],\n",
    "        \"contexts\": [pred[\"context_used\"]],\n",
    "        \"ground_truth\": ref[\"ground_truth_answer\"]\n",
    "    })\n",
    "\n",
    "from datasets import Dataset\n",
    "ragas_dataset = Dataset.from_list(ragas_data)\n",
    "\n",
    "# Evaluate\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=chat_judge_model, temperature=0)\n",
    "\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness\n",
    ")\n",
    "from ragas import evaluate\n",
    "\n",
    "evaluation_scores = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=[answer_relevancy, context_precision, context_recall, faithfulness],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "print(evaluation_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
