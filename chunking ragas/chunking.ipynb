{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec96d834",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import voyageai as vo  \n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "client_vo = vo.Client(api_key=os.getenv(\"VOYAGE_API_KEY\"))\n",
    "chat_evaluator_model = \"gpt-5-nano\"\n",
    "chat_judge_model = \"gpt-4.1-mini\"\n",
    "embedding_model = \"voyage-3-large\"\n",
    "\n",
    "k = 10\n",
    "a = 0.6 \n",
    "lambda_parameter = 0.7 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7594c9a0",
   "metadata": {},
   "source": [
    "**Parameters**  \n",
    "The generator model is lightweight to keep token usage low while still producing good answers. The judge model is intentionally stronger to ensure reliable, consistent scoring.  \n",
    "\n",
    "- `k` = number of top retrieved chunks passed to the generator  \n",
    "- `a` = hybrid search weighting factor (balances dense vs. BM25 scores)  \n",
    "- `lambda_parameter` = MMR diversity–relevance tradeoff during retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c38e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from chunking_cts import chunk_texts\n",
    "# from chunking_rcts import chunk_texts \n",
    "from chunking_semantic import chunk_texts \n",
    "\n",
    "# from chunking_contextual import build_contextual_chunks\n",
    "# chunk_texts, chunk_embeddings = build_contextual_chunks( client_vo=client_vo, embedding_model=embedding_model)\n",
    "print(f\"Number of chunks created: {len(chunk_texts)}\")\n",
    "print(f\"Sample chunk: {chunk_texts[3]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d13b04",
   "metadata": {},
   "source": [
    "**CTS (Character Token Splitting)**  \n",
    "Splits text strictly by character or token limits, producing uniform chunks without considering structure or meaning. Design choices include chunk size (evaluated later), chunk overlap, and the encoding model — here using the widely adopted and balanced `cl100k_base`.\n",
    "\n",
    "**RCTS (Recursive Character Splitting)**  \n",
    "Splits text using a hierarchical fallback approach that respects natural boundaries (headings, paragraphs, sentences) before defaulting to fixed-length cuts. Design choices mirror CTS, mainly chunk size, overlap, and encoding.\n",
    "\n",
    "**Semantic Chunking**  \n",
    "Uses embedding similarity to identify natural semantic boundaries, grouping sentences or small units that share meaning rather than relying on raw length. Design choices include the threshold, this required experimentation where I found that 0.3 produces around 108 chunks. \n",
    "\n",
    "**Contextualized Chunking**  \n",
    "Embeds groups of chunks together and expands each chunk by merging it with its top-k most similar neighbours, resulting in richer context and stronger embeddings. Design choices include the group size fed into the model and the number of neighbouring chunks used for expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a932ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#note: no need to use embed endpoint for contextualized chunking\n",
    "# resp = client_vo.contextualized_embed(\n",
    "#     inputs=[[text] for text in chunk_texts],\n",
    "#     model=embedding_model,\n",
    "#     input_type=\"document\",\n",
    "# )\n",
    "# chunk_embeddings = np.array([r.embeddings[0] for r in resp.results], dtype=np.float32)\n",
    "# chunk_embeddings = chunk_embeddings / np.linalg.norm(chunk_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "resp = client_vo.embed(\n",
    "    chunk_texts,  \n",
    "    model=embedding_model,\n",
    "    input_type=\"document\",\n",
    ")\n",
    "chunk_embeddings = np.array(resp.embeddings, dtype=np.float32)\n",
    "chunk_embeddings = chunk_embeddings / np.linalg.norm(chunk_embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfc5613",
   "metadata": {},
   "source": [
    "The contextualized embedding call sends each chunk in its own list to the Voyage model so the model can interpret it as a standalone document and generate richer, context-aware embeddings. After receiving the outputs, each embedding is extracted, converted to a NumPy array, and L2-normalised so all vectors share the same scale for similarity search.\n",
    "\n",
    "The commented alternative uses standard (non-contextual) embeddings i.e. voyage-large-3, where each chunk is embedded independently without the additional context-conditioning step. The normalisation process remains the same, but the embeddings capture only the isolated text rather than context-enhanced meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d118f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag import response_llm, embed_query\n",
    "import json\n",
    "\n",
    "with open(\"qa_long_dataset.json\", \"r\") as f:\n",
    "    qa_data = json.load(f)\n",
    "questions = [q[\"question\"] for q in qa_data]\n",
    "\n",
    "llm_results = response_llm(\n",
    "    questions=questions,\n",
    "    client=client,\n",
    "    chat_model=chat_evaluator_model,\n",
    "    chunk_embeddings=chunk_embeddings,\n",
    "    chunk_texts=chunk_texts,\n",
    "    embed_query=embed_query,\n",
    "    k=k,\n",
    "    alpha=a,\n",
    "    lambda_param=lambda_parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b182fe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"qa_long_dataset.json\", \"r\") as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "ragas_data = []\n",
    "for ref, pred in zip(qa_data, llm_results):\n",
    "    ragas_data.append({\n",
    "        \"question\": ref[\"question\"],\n",
    "        \"answer\": pred[\"answer\"],\n",
    "        \"contexts\": [pred[\"context_used\"]],\n",
    "        \"ground_truth\": ref[\"ground_truth_answer\"]\n",
    "    })\n",
    "\n",
    "from datasets import Dataset\n",
    "ragas_dataset = Dataset.from_list(ragas_data)\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=chat_judge_model, temperature=0)\n",
    "\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness\n",
    ")\n",
    "from ragas import evaluate\n",
    "\n",
    "evaluation_scores = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=[faithfulness, answer_relevancy, context_precision, context_recall],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "print(evaluation_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
